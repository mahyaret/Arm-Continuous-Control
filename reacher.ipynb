{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you're in the right virtual environment and the right python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.8 :: Anaconda, Inc.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Windows'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python --version\n",
    "import platform\n",
    "platform.system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below.  Please run the next code cell without making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import deque\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    env = UnityEnvironment(file_name=\"data/Reacher_Windows_x86_64/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-Critic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_hidden_layer(input_dim, hidden_layers):\n",
    "    \"\"\"Build hidden layer.\n",
    "    Params\n",
    "    ======\n",
    "        input_dim (int): Dimension of hidden layer input\n",
    "        hidden_layers (list(int)): Dimension of hidden layers\n",
    "    \"\"\"\n",
    "    hidden = nn.ModuleList([nn.Linear(input_dim, hidden_layers[0])])\n",
    "    if len(hidden_layers)>1:\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        hidden.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "    return hidden\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,state_size,action_size,shared_layers,\n",
    "                 critic_hidden_layers=[],actor_hidden_layers=[],\n",
    "                 seed=0, init_type=None):\n",
    "        \"\"\"Initialize parameters and build policy.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            shared_layers (list(int)): Dimension of the shared hidden layers\n",
    "            critic_hidden_layers (list(int)): Dimension of the critic's hidden layers\n",
    "            actor_hidden_layers (list(int)): Dimension of the actor's hidden layers\n",
    "            seed (int): Random seed\n",
    "            init_type (str): Initialization type\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.init_type = init_type\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.sigma = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "        # Add shared hidden layer\n",
    "        self.shared_layers = build_hidden_layer(input_dim=state_size,\n",
    "                                                hidden_layers=shared_layers)\n",
    "\n",
    "        # Add critic layers\n",
    "        if critic_hidden_layers:\n",
    "            # Add hidden layers for critic net if critic_hidden_layers is not empty\n",
    "            self.critic_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                    hidden_layers=critic_hidden_layers)\n",
    "            self.critic = nn.Linear(critic_hidden_layers[-1], 1)\n",
    "        else:\n",
    "            self.critic_hidden = None\n",
    "            self.critic = nn.Linear(shared_layers[-1], 1)\n",
    "\n",
    "        # Add actor layers\n",
    "        if actor_hidden_layers:\n",
    "            # Add hidden layers for actor net if actor_hidden_layers is not empty\n",
    "            self.actor_hidden = build_hidden_layer(input_dim=shared_layers[-1],\n",
    "                                                   hidden_layers=actor_hidden_layers)\n",
    "            self.actor = nn.Linear(actor_hidden_layers[-1], action_size)\n",
    "        else:\n",
    "            self.actor_hidden = None\n",
    "            self.actor = nn.Linear(shared_layers[-1], action_size)\n",
    "\n",
    "        # Apply Tanh() to bound the actions\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Initialize hidden and actor-critic layers\n",
    "        if self.init_type is not None:\n",
    "            self.shared_layers.apply(self._initialize)\n",
    "            self.critic.apply(self._initialize)\n",
    "            self.actor.apply(self._initialize)\n",
    "            if self.critic_hidden is not None:\n",
    "                self.critic_hidden.apply(self._initialize)\n",
    "            if self.actor_hidden is not None:\n",
    "                self.actor_hidden.apply(self._initialize)\n",
    "\n",
    "    def _initialize(self, n):\n",
    "        \"\"\"Initialize network weights.\n",
    "        \"\"\"\n",
    "        if isinstance(n, nn.Linear):\n",
    "            if self.init_type=='xavier-uniform':\n",
    "                nn.init.xavier_uniform_(n.weight.data)\n",
    "            elif self.init_type=='xavier-normal':\n",
    "                nn.init.xavier_normal_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-uniform':\n",
    "                nn.init.kaiming_uniform_(n.weight.data)\n",
    "            elif self.init_type=='kaiming-normal':\n",
    "                nn.init.kaiming_normal_(n.weight.data)\n",
    "            elif self.init_type=='orthogonal':\n",
    "                nn.init.orthogonal_(n.weight.data)\n",
    "            elif self.init_type=='uniform':\n",
    "                nn.init.uniform_(n.weight.data)\n",
    "            elif self.init_type=='normal':\n",
    "                nn.init.normal_(n.weight.data)\n",
    "            else:\n",
    "                raise KeyError('initialization type is not found in the set of existing types')\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> (action, value).\"\"\"\n",
    "        def apply_multi_layer(layers,x,f=F.leaky_relu):\n",
    "            for layer in layers:\n",
    "                x = f(layer(x))\n",
    "            return x\n",
    "\n",
    "        state = apply_multi_layer(self.shared_layers,state)\n",
    "\n",
    "        v_hid = state\n",
    "        if self.critic_hidden is not None:\n",
    "            v_hid = apply_multi_layer(self.critic_hidden,v_hid)\n",
    "\n",
    "        a_hid = state\n",
    "        if self.actor_hidden is not None:\n",
    "            a_hid = apply_multi_layer(self.actor_hidden,a_hid)\n",
    "\n",
    "        a = self.tanh(self.actor(a_hid))\n",
    "        value = self.critic(v_hid).squeeze(-1)\n",
    "        return a, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = env_info.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  7.90150833e+00 -1.00000000e+00\n",
      "  1.25147629e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.22214413e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200, nrand=5, train_mode=False):\n",
    "\n",
    "    def to_tensor(x, dtype=np.float32):\n",
    "        return torch.from_numpy(np.array(x).astype(dtype)).to(device)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "    value_list=[]\n",
    "    done_list=[]\n",
    "\n",
    "    env_info = envs.reset(train_mode=train_mode)[brain_name]\n",
    "\n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        action = np.random.randn(num_agents, action_size)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        env_info = envs.step(action)[brain_name]\n",
    "\n",
    "    for t in range(tmax):\n",
    "        states = to_tensor(env_info.vector_observations)\n",
    "        action_est, values = policy(states)\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        log_probs = torch.sum(log_probs, dim=-1).detach()\n",
    "        values = values.detach()\n",
    "        actions = actions.detach()\n",
    "\n",
    "        env_actions = actions.cpu().numpy()\n",
    "        env_info = envs.step(env_actions)[brain_name]\n",
    "        rewards = to_tensor(env_info.rewards)\n",
    "        dones = to_tensor(env_info.local_done, dtype=np.uint8)\n",
    "\n",
    "        state_list.append(states.unsqueeze(0))\n",
    "        prob_list.append(log_probs.unsqueeze(0))\n",
    "        action_list.append(actions.unsqueeze(0))\n",
    "        reward_list.append(rewards.unsqueeze(0))\n",
    "        value_list.append(values.unsqueeze(0))\n",
    "        done_list.append(dones.unsqueeze(0))\n",
    "        #if np.any(dones.cpu().numpy()):\n",
    "        if np.any(dones.numpy()):\n",
    "            env_info = envs.reset(train_mode=train_mode)[brain_name]\n",
    "\n",
    "    state_list = torch.cat(state_list, dim=0)\n",
    "    prob_list = torch.cat(prob_list, dim=0)\n",
    "    action_list = torch.cat(action_list, dim=0)\n",
    "    reward_list = torch.cat(reward_list, dim=0)\n",
    "    value_list = torch.cat(value_list, dim=0)\n",
    "    done_list = torch.cat(done_list, dim=0)\n",
    "    return prob_list, state_list, action_list, reward_list, value_list, done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(rewards, values, dones):\n",
    "    n_step = len(rewards)\n",
    "    n_agent = len(rewards[0])\n",
    "\n",
    "    # Create empty buffer\n",
    "    GAE = torch.zeros(n_step,n_agent).float().to(device)\n",
    "    returns = torch.zeros(n_step,n_agent).float().to(device)\n",
    "\n",
    "    # Set start values\n",
    "    GAE_current = torch.zeros(n_agent).float().to(device)\n",
    "\n",
    "    TAU = 0.95\n",
    "    discount = 0.99\n",
    "    values_next = values[-1].detach()\n",
    "    returns_current = values[-1].detach()\n",
    "    for irow in reversed(range(n_step)):\n",
    "        values_current = values[irow]\n",
    "        rewards_current = rewards[irow]\n",
    "        gamma = discount * (1. - dones[irow].float())\n",
    "\n",
    "        # Calculate TD Error\n",
    "        td_error = rewards_current + gamma * values_next - values_current\n",
    "        # Update GAE, returns\n",
    "        GAE_current = td_error + gamma * TAU * GAE_current\n",
    "        returns_current = rewards_current + gamma * returns_current\n",
    "        # Set GAE, returns to buffer\n",
    "        GAE[irow] = GAE_current\n",
    "        returns[irow] = returns_current\n",
    "\n",
    "        values_next = values_current\n",
    "\n",
    "    return GAE, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(envs, policy, tmax=1000):\n",
    "    reward_list=[]\n",
    "    env_info = envs.reset(train_mode=False)[brain_name]\n",
    "    for t in range(tmax):\n",
    "        states = torch.from_numpy(env_info.vector_observations).float().to(device)\n",
    "        action_est, values = policy(states)\n",
    "        sigma = nn.Parameter(torch.zeros(action_size))\n",
    "        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "        actions = dist.sample()\n",
    "        env_actions = actions.cpu().numpy()\n",
    "        env_info = envs.step(env_actions)[brain_name]\n",
    "        reward = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        reward_list.append(np.mean(reward))\n",
    "\n",
    "        # stop if any of the trajectories is done to have retangular lists\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "An actor-critic structure with continuous action space is used for this project. The policy consists of 3 parts, a shared hidden layers, actor, and critic.\n",
    "The actor layer outputs the mean value of a normal distribution, from which the agent's action is sampled. The critic layer yields the value function.\n",
    "\n",
    "- Shared layer:\n",
    "```\n",
    "Input State(33) -> Dense(128) -> LeakyReLU -> Dense(128) -> LeakyReLU*\n",
    "```\n",
    "- Actor and Critic layers:\n",
    "```\n",
    "LeakyRelu* -> Dense(64) -> LeakyRelu -> Dense(4)-> tanh -> Actor's output\n",
    "LeakyReLU* -> Dense(64) -> LeakyRelu -> Dense(1) -> Critic's output\n",
    "```\n",
    "\n",
    "### Model update using PPO/GAE\n",
    "The hyperparameters used during training are:\n",
    "\n",
    "Parameter | Value | Description\n",
    "------------ | ------------- | -------------\n",
    "Number of Agents | 20 | Number of agents trained simultaneously\n",
    "Episodes | 2000 | Maximum number of training episodes\n",
    "tmax | 1000 | Maximum number of steps per episode\n",
    "Epochs | 10 | Number of training epoch per batch sampling\n",
    "Batch size | 128*20 | Size of batch taken from the accumulated  trajectories\n",
    "Discount (gamma) | 0.99 | Discount rate \n",
    "Epsilon | 0.1 | Ratio used to clip r = new_probs/old_probs during training\n",
    "Gradient clip | 10.0 | Maximum gradient norm \n",
    "Beta | 0.01 | Entropy coefficient \n",
    "Tau | 0.95 | tau coefficient in GAE\n",
    "Learning rate | 2e-4 | Learning rate \n",
    "Optimizer | Adam | Optimization method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your own policy!\n",
    "policy=ActorCritic(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              shared_layers=[128, 64],\n",
    "              critic_hidden_layers=[],\n",
    "              actor_hidden_layers=[],\n",
    "              init_type='xavier-uniform',\n",
    "              seed=0).to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, average score: 0.74\n",
      "Episode: 20, average score: 0.79\n",
      "Episode: 30, average score: 0.91\n",
      "Episode: 40, average score: 0.99\n",
      "Episode: 50, average score: 1.10\n",
      "Episode: 60, average score: 1.19\n",
      "Episode: 70, average score: 1.33\n",
      "Episode: 80, average score: 1.45\n",
      "Episode: 90, average score: 1.60\n",
      "Episode: 100, average score: 1.77\n",
      "Episode: 110, average score: 2.05\n",
      "Episode: 120, average score: 2.38\n",
      "Episode: 130, average score: 2.75\n",
      "Episode: 140, average score: 3.18\n",
      "Episode: 150, average score: 3.78\n",
      "Episode: 160, average score: 4.55\n",
      "Episode: 170, average score: 5.28\n",
      "Episode: 180, average score: 6.23\n",
      "Episode: 190, average score: 7.24\n",
      "Episode: 200, average score: 8.33\n",
      "Episode: 210, average score: 9.54\n",
      "Episode: 220, average score: 10.78\n",
      "Episode: 230, average score: 12.04\n",
      "Episode: 240, average score: 13.28\n",
      "Episode: 250, average score: 14.40\n",
      "Episode: 260, average score: 15.45\n",
      "Episode: 270, average score: 16.57\n",
      "Episode: 280, average score: 17.58\n",
      "Episode: 290, average score: 18.48\n",
      "Episode: 300, average score: 19.28\n",
      "Episode: 310, average score: 20.08\n",
      "Episode: 320, average score: 20.83\n",
      "Episode: 330, average score: 21.55\n",
      "Episode: 340, average score: 22.27\n",
      "Episode: 350, average score: 22.94\n",
      "Episode: 360, average score: 23.60\n",
      "Episode: 370, average score: 24.23\n",
      "Episode: 380, average score: 24.83\n",
      "Episode: 390, average score: 25.51\n",
      "Episode: 400, average score: 26.23\n",
      "Episode: 410, average score: 26.87\n",
      "Episode: 420, average score: 27.48\n",
      "Episode: 430, average score: 28.09\n",
      "Episode: 440, average score: 28.66\n",
      "Episode: 450, average score: 29.28\n",
      "Episode: 460, average score: 29.84\n",
      "Environment solved in 463 episodes!\tAverage Score: 30.02\n",
      "Average Score: 30.02\n",
      "Elapsed time: 1:08:33.381600\n",
      "Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "discount = 0.993\n",
    "epsilon = 0.07\n",
    "beta = .01\n",
    "opt_epoch = 10\n",
    "episode = 2000\n",
    "batch_size = 128\n",
    "tmax = 1000 #env episode steps\n",
    "save_scores = []\n",
    "print_per_n = min(10,episode/10)\n",
    "counter = 0\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for e in range(episode):\n",
    "    policy.eval()\n",
    "    old_probs_lst, states_lst, actions_lst, rewards_lst, values_lst, dones_list = collect_trajectories(envs=env,\n",
    "                                                                                                       policy=policy,\n",
    "                                                                                                       tmax=tmax,\n",
    "                                                                                                       nrand = 0,\n",
    "                                                                                                       train_mode=True)\n",
    "\n",
    "    avg_score = rewards_lst.sum(dim=0).mean().item()\n",
    "    scores_window.append(avg_score)\n",
    "    save_scores.append(avg_score)\n",
    "    \n",
    "    gea, target_value = calc_returns(rewards = rewards_lst,\n",
    "                                     values = values_lst,\n",
    "                                     dones=dones_list)\n",
    "    gea = (gea - gea.mean()) / (gea.std() + 1e-8)\n",
    "\n",
    "    policy.train()\n",
    "\n",
    "    # cat all agents\n",
    "    def concat_all(v):\n",
    "        if len(v.shape) == 3:\n",
    "            return v.reshape([-1, v.shape[-1]])\n",
    "        return v.reshape([-1])\n",
    "\n",
    "    old_probs_lst = concat_all(old_probs_lst)\n",
    "    states_lst = concat_all(states_lst)\n",
    "    actions_lst = concat_all(actions_lst)\n",
    "    rewards_lst = concat_all(rewards_lst)\n",
    "    values_lst = concat_all(values_lst)\n",
    "    gea = concat_all(gea)\n",
    "    target_value = concat_all(target_value)\n",
    "\n",
    "    # gradient ascent step\n",
    "    n_sample = len(old_probs_lst)//batch_size\n",
    "    idx = np.arange(len(old_probs_lst))\n",
    "    np.random.shuffle(idx)\n",
    "    for epoch in range(opt_epoch):\n",
    "        for b in range(n_sample):\n",
    "            ind = idx[b*batch_size:(b+1)*batch_size]\n",
    "            g = gea[ind]\n",
    "            tv = target_value[ind]\n",
    "            actions = actions_lst[ind]\n",
    "            old_probs = old_probs_lst[ind]\n",
    "\n",
    "            action_est, values = policy(states_lst[ind])\n",
    "            sigma = nn.Parameter(torch.zeros(action_size))\n",
    "            dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            log_probs = torch.sum(log_probs, dim=-1)\n",
    "            entropy = torch.sum(dist.entropy(), dim=-1)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_probs)\n",
    "            ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            L_CLIP = torch.mean(torch.min(ratio*g, ratio_clipped*g))\n",
    "            # entropy bonus\n",
    "            S = entropy.mean()\n",
    "            # squared-error value function loss\n",
    "            L_VF = 0.5 * (tv - values).pow(2).mean()\n",
    "            # clipped surrogate\n",
    "            L = -(L_CLIP - L_VF + beta*S)\n",
    "            optimizer.zero_grad()\n",
    "            # This may need retain_graph=True on the backward pass\n",
    "            # as pytorch automatically frees the computational graph after\n",
    "            # the backward pass to save memory\n",
    "            # Without this, the chain of derivative may get lost\n",
    "            L.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            del(L)\n",
    "\n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.998\n",
    "    \n",
    "    # display some progress every n iterations\n",
    "    if (e+1)%print_per_n ==0 :\n",
    "        print(\"Episode: {0:d}, average score: {1:.2f}\".format(e+1,np.mean(scores_window)), end=\"\\n\")\n",
    "    else:\n",
    "        print(\"Episode: {0:d}, score: {1:.2f}\".format(e+1, avg_score), end=\"\\r\")\n",
    "    if np.mean(scores_window)<5.0:\n",
    "        counter = 0# stop if any of the trajectories is done to have retangular lists\n",
    "    if e>=25 and np.mean(scores_window)>30.0:\n",
    "        print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e+1, np.mean(scores_window)))\n",
    "        break\n",
    "\n",
    "\n",
    "print('Average Score: {:.2f}'.format(np.mean(scores_window)))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Elapsed time: {}\".format(timedelta(seconds=elapsed)))\n",
    "print(\"Saving checkpoint!\")\n",
    "# save your policy!\n",
    "torch.save(policy.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(save_scores)), save_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, score: 31.514999295584857\n",
      "Episode: 2, score: 31.991499284934253\n",
      "Episode: 3, score: 32.324832610817005\n",
      "Episode: 4, score: 32.35762427675073\n",
      "Episode: 5, score: 32.52949927290902\n",
      "Episode: 6, score: 32.232415946216015\n",
      "Episode: 7, score: 32.204713565882834\n",
      "Episode: 8, score: 32.191436780465295\n",
      "Episode: 9, score: 32.14727705923012\n",
      "Episode: 10, score: 32.136749281687656\n"
     ]
    }
   ],
   "source": [
    "episode = 10\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "# load the model\n",
    "policy.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "# evaluate the model\n",
    "for e in range(episode):\n",
    "    rewards = eval_policy(envs=env, policy=policy, tmax=1000)\n",
    "    total_rewards = np.sum(rewards,0)\n",
    "    scores_window.append(total_rewards.mean())\n",
    "    print(\"Episode: {0:d}, score: {1}\".format(e+1, np.mean(scores_window)), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "1. Hyperparameter optimization and analysis.\n",
    "2. Studying the effect of clipping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
